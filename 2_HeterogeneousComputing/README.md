# 2. Heterogeneous Computing
CUDA 编程涉及在两个不同的平台上同时运行代码：一个主机系统包含一个或多个 CPU，以及一个或多个支持 CUDA 的 NVIDIA GPU 设备。

尽管 NVIDIA GPU 经常与图形处理相关联，但它们也是强大的算术引擎，能够并行运行成千上万个轻量级线程。这种能力使它们非常适合可以利用并行执行的计算。

然而，GPU 设备的设计与主机系统有着显著的不同，理解这些差异以及它们如何决定 CUDA 应用程序的性能，对于有效使用 CUDA 非常重要。

## 2.1. Differences between Host and Device
主要差异在于线程模型和独立的物理内存：
* Threading resourcesThreads \
    主机系统的执行管道只能支持有限数量的并发线程。例如，具有两个32核处理器的服务器只能同时运行64个线程（如果 CPU 支持同时多线程，可以运行更多线程）。相比之下，CUDA 设备上最小的可执行并行单元由32个线程组成（称为线程束）。现代 NVIDIA GPU 每个多处理器可以支持最多2048个活动线程（参见 CUDA C++ 编程指南中的功能和规格）。在具有80个多处理器的 GPU 上，这可以支持超过160,000个同时活动的线程。

* Threads \
    CPU上的线程通常是重量级实体。操作系统必须将线程从CPU执行通道上换进换出，以提供多线程功能。因此，线程切换（当两个线程被交换时）是缓慢且昂贵的。相比之下，GPU上的线程极其轻量。在典型系统中，成千上万个线程排队等待工作（每32个线程组成一个线程束）。如果GPU必须等待一个线程束，它只需开始执行另一个线程束的工作。由于为所有活动线程分配了单独的寄存器，当在GPU线程之间切换时，无需交换寄存器或其他状态。资源保持分配给每个线程，直到其完成执行。总之，CPU核心设计用于减少少量线程的延迟，而GPU则设计用于处理大量并发的轻量级线程，以最大化吞吐量。

* RAM \
    主机系统和设备各自拥有独立的物理内存。由于主机和设备内存是分开的，主机内存中的项目必须偶尔在设备内存和主机内存之间传输，如"What Runs on a CUDA-Enabled Device?."中所述。

这些是 CPU 主机和 GPU 设备在并行编程方面的主要硬件差异。其他差异在本文档的其他地方有讨论。考虑到这些差异，编写的应用程序可以将主机和设备视为一个统一的异构系统，其中每个处理单元都用于其最擅长的工作：host上的顺序工作和device上的并行工作。

## 2.2. What Runs on a CUDA-Enabled Device?
在确定应用程序的哪些部分运行在设备上时，应考虑以下问题：
1. device 非常适合可以同时在多个数据元素上并行运行的计算。这通常涉及对大型数据集（如矩阵）进行算术运算，其中相同的操作可以同时对数千，甚至数百万个元素执行。这是 CUDA 良好性能的要求：软件必须使用大量（通常是数千或数万个）并发线程。对运行众多线程并行的支持源于上面描述的 CUDA 轻量级线程模型。

2. 要使用 CUDA，数据值必须从主机传输到 device。就性能而言，这些传输成本很高，应该尽量减少。（参见 " Data Transfer Between Host and Device"）这种成本有以下几个影响：\
    * 操作的复杂性应证明将数据来回传输到 device 的成本是合理的。仅为少量线程短暂使用而传输数据的代码几乎不会带来性能收益。理想的情况是许多线程执行大量工作。\
    例如，将两个矩阵传输到 device 以执行矩阵加法，然后将结果传输回主机，不会带来太多性能收益。这里的问题是每个传输的数据元素执行的操作次数。对于上述过程，假设矩阵大小为 NxN，有 N² 次操作（加法）和 3N² 个传输元素，因此操作与传输元素的比率为 1:3 或 O(1)。当这个比率更高时，更容易实现性能收益。例如，相同矩阵的矩阵乘法需要 N³ 次操作（乘加），因此操作与传输元素的比率为 O(N)，在这种情况下，矩阵越大，性能收益越大。操作类型也是一个额外因素，因为加法与三角函数等操作的复杂性不同。在确定操作应在主机还是 device 上执行时，重要的是包括将数据来回传输到 device 的开销。
    * 数据应尽可能长时间地保留在 device 上。由于应尽量减少数据传输，运行多个内核的程序应在内核调用之间将数据保留在 device 上，而不是将中间结果传输到主机，然后再发送回 device 进行后续计算。因此，在前面的例子中，如果要添加的两个矩阵已经作为某些先前计算的结果存在于 device 上，或者如果加法的结果将在后续计算中使用，那么矩阵加法应该在 device 上本地执行。即使计算序列中的某一步可以在主机上更快地执行，也应该采用这种方法。如果它避免了主机和 device 内存之间的一次或多次传输，即使是相对较慢的内核也可能是有利的。主机和 device 之间的数据传输提供了进一步的详细信息，包括主机和 device 之间的带宽测量与 device 内部带宽的比较。

为了获得最佳性能，device 上运行的相邻线程在内存访问方面应该有一定的一致性。某些内存访问模式使硬件能够将多个数据项的读写操作合并为一个操作。如果数据不能被组织成能进行合并的形式，或没有足够的局部性以有效使用 L1 缓存或纹理缓存，那么在 GPU 上进行计算时，速度提升会较小。一个显著的例外是完全随机的内存访问模式。一般来说，应该避免这些模式，因为与峰值能力相比，任何架构处理这些内存访问模式的效率都很低。然而，与基于缓存的架构（如 CPU）相比，隐藏延迟架构（如 GPU）往往更能应对完全随机的内存访问模式。

```
在具有集成 GPU 的片上系统（如 NVIDIA® Tegra®）上，主机和 device 内存在物理上是相同的，但主机和 device 内存之间仍然存在逻辑上的区别。有关详细信息，请参见《Application Note on CUDA for Tegra》。 https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/
```